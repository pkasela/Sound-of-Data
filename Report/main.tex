%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Article
% LaTeX Template
% Version 2.0 (28/2/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)

\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}

\input{structure.tex} % Specifies the document structure and loads requires packages

% \usepackage{textcomp}
%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{Sound of Data} % The article title

\author{
  \authorstyle{
    % DO NOT TOUCH HSPACEs!! @Momo
    Riccardo Cervero \hspace{25pt} 000000 \\  % x + 4
    Marco Ferrario \hspace{44pt}  000000 \\   % x + 23
    Pranav Kasela \hspace{46.5pt} 000000 \\   % x + 25.5
    Federico Moiraghi \hspace{21pt} 799735    % x
  } % Authors
  \newline\newline
  \institution{Università degli Studi di Milano Bicocca}
}

\date{Anno Accademico 2018/19} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{Obiettivo del progetto è analizzare la discussione mediatica riguardante i soggetti del mondo musicale contemporaneo e passato.
}
\hfill
\newpage

\tableofcontents
\hfill
\newpage

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
\part{Introduzione}
Per raggiungere i traguardi posti dal progetto \textit{Sound of Data}\footnote{\url{https://github.com/pkasela/Sound-of-Data}}, si son dovuti raccogliere dati sufficienti per costruire un \textit{knowledge graph} adeguato alla materia: scaricato un \textit{dump} di \url{musicbrainz.org} come database relazionale (già esportato in formato \textit{Tabular Separated Values} dai manutentori), si è dapprima importato in Apache Hadoop\footnote{\url{https://hadoop.apache.org}} per effettuare una rapida pulizia preliminare e infine esportato in modo tale da costruire un grafo Neo4J\footnote{\url{https://neo4j.com}}.
Costruita quindi la base di conoscenza su cui operare, si sono raccolti \textit{tweet} in tempo reale grazie ad Apache Kafka\footnote{\url{https://kafka.apache.org}}, per poi analizzarli in automatico con un rudimentale strumento di \textit{instance matching}.

\hfill
\newpage
\part{Costruzione dello \textit{knowledge graph}}

\section{Data Cleaning con python e Pig}
I dati vengono scaricati dal sito di musicbrainz attraverso il file get\_data.py, che oltre a scaricare i dati effettua un'analisi preliminare di sostituire tutti i caratteri "\\N" con campi vuoti in tutti i file attraverso il comando sed e salva solo i file necessari per il database a grafo che si vuole creare. 
La lista dei generi è presente sempre sul sito di musicbrainz, con uno scraper viene salvato nel codice e confrontato con il file tag e vengono eliminati tutti i tag che non sono dei generi.

%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%% Ma lo schema iniziale va messo?
%%%%%%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%

I file vengono trasferiti su HDFS per essere processati, in particolare usiamo Apache Pig, che è una piattaforma per processare i dati, la decisione di utilizzare Pig piuttosto che SQL è dovuta al fatto che il caricamento dei dati in SQL è molto lenta e inoltre Pig riesce a sfruttare la potenza di HDFS.
L'engine usato con Pig è Tez, che rende molto più veloce l'esecuzione del processamento dei dati saltando diverse fasi di scrittura dei risultati parziali su HDFS.
%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%% Dopo cercare e mettere il funzionamento di Tez
%%%% Non vorrei scrivere cavolate quindi dopo cerco bene
%%%%%%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%

Una volta finita la pulizia i dati vengono trasferiti fuori da HDFS sul filesystem, e vengono unite i risultati di pig con il comando cat. A questo punto i dati sono pronti per essere caricati su Neo4j.

%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%% Conviene mettere lo schema finale delle tabelle (dopo lo metto)
%%%%%%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%

\section{Neo4J}

I dati puliti da Pig vengono caricati su neo4j usando neo4j-import.
%cosa altro si deve scrivere hahaha

\hfill
\newpage
\part{Analisi dei tweet}
\section{Elaborazione mediante Apache Kafka}
Nella fase di analisi in tempo reale, i tweet vengono filtrati ed elaborati attraverso una sequenza di operazioni. Innanzitutto, essi vengono estratti grazie all'API \textit{Tweepy} per Python, messa a disposizione da Twitter, utilizzando come parole chiave di ricerca 400 generi musicali fra i 419 ottenuti dallo scraping della pagina \textit{'https://musicbrainz.org/genres'} e filtrando soltanto i testi pubblicati in lingua italiana. Poi, all'interno della class \textit{Listener}, la funzione \textit{tweet\_preparations} elabora in vari step il JSON di ogni tweet ricevuto. La prima fase coincide con l'estrazione dello \textit{"screen name"} dell'utente, del contenuto postato e dell'ora e data precisa di pubblicazione. In particolare, il testo viene modificato in modo da rendere leggibili i caratteri speciali e i cosiddetti \textit{escape characters}. La seconda fase sfrutta l'API di \textit{Botometer} per calcolare la probabilità - definita \textit{"score"} - che un profilo sia in realtà gestito da un BOT, osservandone il comportamento passato. Poiché questo tipo di gestione automatizzata degli account altera, spesso fortemente, la discussione mediatica, aumentando arbitrariamente il flusso di determinati contenuti, si è deciso di non archiviare i tweet appartenenti a tali profili. Nel dettaglio, se lo score associato all'utente di ogni tweet supera una soglia fissata al 90\%, lo \textit{screen name} viene memorizzato all'interno di una \textit{blacklist}, altrimenti smistato in una \textit{whitelist}, in modo che il programma possa riconoscere più velocemente ogni profilo ed evitare inutili ricalcoli. Per aumentare l'efficienza di questo storage e garantire la scalabilità, si è scelto di adoperare il \textit{data store Riak}, di tipo \textit{NoSQL Key-Value}, attraverso la propria libreria in Python. La terza fase verifica che il testo sia effettivamente legato al tema musicale. La quarta e ultima fase consiste nel riconoscimento delle parole riguardanti il contesto musicale, aggiungendo al dizionario prodotto dalle fasi precedenti tre chiavi corrispondenti rispettivamente alla lista di artisti, album o canzoni estratti dal testo analizzato. Questi due step sono permessi dall'uso dell'API fornita dal sito \textit{musicbrainz.org}. Per maggiori dettagli sul processo di analisi semantica, si rimanda alla sezione dedicata. Infine, nel caso in cui si verifichino le due condizioni note - cioè che l'utente non abbia probabilità elevata di essere un BOT e che il testo pubblicato si riferisca effettivamente al mondo della musica, ogni risultato della funzione \textit{tweet\_preparations} viene ricodificato e passato come messaggio ad un \textit{producer} di \textit{Apache Kafka}, inizializzato mediante la libreria \textit{Kafka-python}, ed inviato alla \textit{topic} nominata \textit{"Music\_Tweets"}. Grazie all'utilizzo di questa piattaforma, il flusso di \textit{feed} viene gestito in tempo reale, assicurando bassissima latenza e grande scalabilità. 

\section{Consuming dei Tweets in Neo4j}
Un principale vantaggio di \textit{Apache Kafka} è la capacità di connettersi efficientemente a sistemi esterni, garantendo in tal modo la continuità dello stream di dati da una fonte - il \textit{Listener} di Python -, ad una \textit{"landing zone"}, che in questo caso corrisponde al DBMS \textit{Neo4j}. Si è scelto di effettuare lo \textit{storage} dei tweets in un \textit{GraphDB}, e in particolare \textit{Neo4j}, tra le varie motivazioni, per le sue caratteristiche di estrema scalabilità, efficienza nella gestione, elevata capacità di adattamento al fenomeno di studio e interpretabilità dei dati. L'esportazione diretta dei tweets da \textit{Apache Kafka} all'interno del \textit{data store} avviene grazie al \textit{plug-in} di \textit{Neo4j} chiamato \textit{"Neo4j Streams Consumer"}, un'applicazione che effettua un'ingestione diretta e automatizzata all'interno di \textit{Neo4j}, permettendo la lettura dei dati presenti nella \textit{topic} identicamente a qualsiasi applicazione \textit{Consumer} di \textit{Kafka}. \textit{Neo4j Streams Consumer} permette all'utente di specificare arbitrariamente le relazioni, entità e proprietà in cui i \textit{payloads} di \textit{Apache Kafka}, corrispondenti al JSON di ciascun tweet importato, dovranno essere organizzati per costruire progressivamente il grafo. La struttura di quest'ultimo viene dichiarata attraverso un \textit{Cypher template}, ovvero un insieme di \textit{queries} semantiche di \textit{Cypher}, all'interno di un file di configurazione, che nel caso presentato è il \textit{"docker-compose.yml"}, poiché il progetto viene eseguito mediante \textit{Docker Desktop}, in quanto quest'ultimo presenta il vantaggio di riuscire a far comunicare automaticamente e in maniera trasparente i containers coinvolti. Una volta specificato il \textit{Cypher template}, installato il \textit{plugin} all'interno del \textit{Docker}, e successivamente montati i container e i relativi collegamenti, la fase di mera esecuzione del progetto avviene mediante la funzione denominata \textit{"streams.consume"}, la quale crea immediatamente la struttura del grafo come indicato. Nello specifico, all'arrivo di un tweet verranno dunque aggiunti i nodi \textit{"Tweet"} e \textit{"User"}. Il primo conterrà le seguenti \textit{property keys}: il testo \textit{"text"}, lo \textit{"screen\_name"} e \textit{"created\_at}, cioè ora e data di pubblicazione. Il secondo, invece, conterrà solamente la proprietà \textit{"name"}, che riporta lo \textit{screen name}. Tra i due nuovi nodi verrà generata la relazione \textit{"BELONGS\_TO"}, ad indicare il profilo di appartenenza di ciascun post. In più, il programma effettuerà automaticamente una \textit{merge} della coppia appena generata con i nodi già presenti all'interno del grafo, importati precedentemente da \textit{musicbrainz.org}, nella seguente maniera: quando il testo cita, ad esempio, un artista, il software crea la relazione \textit{"TALKS\_ABOUT",} rappresentata da un arco connesso al nodo a cui appartiene il \textit{"value"} di quel preciso artista - cioè il suo id -, il quale sarà già a sua volta collegato ai generi musicali cui appartengono le proprie produzioni. Lo stesso accade per quanto riguarda le \textit{labels} relative al titolo di una canzone, oppure di un album. A questo punto, il grafo apparirà direttamente cosparso di nodi utente, raggruppati in comunità a seconda dell'oggetto dei propri tweet, ingrandendosi e variando in base alle dinamiche interne alla discussione mediatica nell'arco di vari cicli orari e giornalieri. Grazie a questa semplice struttura, l'analisi avviene già in maniera grafica e trasparente, e l'utente è in grado di distinguere la grandezza di queste comunità variabili, intendere quali legami esistano fra di esse, e quanto questi siano intensi. Per ottenere queste informazioni, è infatti sufficiente scaricare un \textit{dump} del grafo a intervalli regolari e confrontare i vari risultati. Il risultato finale, dall'interfaccia web di \texti{Neo4j}, apparirà come segue:
#Screenshot

\section{Riconoscimento delle istanze nel testo}
Data la mole di tweet scaricabili, si è deciso di costruire un strumento di \textit{instance matching} creato \textit{ad hoc} per i tweet.
Vista la scarsa abilità di algoritmi basati su reti neurali e \textit{deep learning} ad analizzare il breve (e quindi decontestualizzato) testo di un tweet, si è costruito un modello per l'identificazione di ipotetiche entità su cui basarsi per confronti col database (grazie alla API\footnote{\url{https://python-musicbrainzngs.readthedocs.io/en/v0.6/}} offerta da \url{musicbrainz.org} stesso).

\subsection{Identificazione delle entità}
Le entità sono riconosciute non mediante \textit{machine learning} ma grazie a semplici stratagemmi linguistici.
Prima di tutto il testo del tweet è ripulito da eventuali abbreviazioni gergali; subito dopo sono ricercate le parole nel testo che non risultano essere italiane: analizzando tweet in lingua italiana, si presume che una stringa in lingua diversa abbia una certa importanza.
Per fare questo è usato un mero correttore ortografico che evidenzia quali parole non sono riconosciute; di aiuto nel compito è anche una semplice espressione regolare che tenta di stabilire quali parole non seguono la costruzione sillabica italiana (rientrando quindi o nella categoria dei sostantivi della quinta classe o, nei casi fortunati, nelle entità cercate): secondo le regole linguistiche, una sillaba correttamente formata è composta da un numero massimo di tre consonanti seguita da una vocale e da al più una sola consonante (o vocale con suono consonantico, formando quindi un dittongo).
Alle entità così individuate si aggiungono tutte le parole scritte in maiuscolo (che in un testo di così bassa formalità non sempre coincidono coi nomi propri), anche se a inizio frase.
Grazie all'uso della punteggiatura (le virgolette) e delle preposizioni, si tenta inoltre di stabilire se l'entità rilevata è un presunto autore o una presunta opera e quindi cercata all'interno del database.

\subsubsection{Prestazioni del modello}

\subsubsection{Margini di miglioramento}

\subsection{Riconoscimento delle entità}

\hfill
\newpage
\part{Visualizzazioni dei dati}

\hfill
\newpage
\part{Risultati e conclusioni}



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\hfill
\newpage
\printbibliography[title={Bibliografia}]

%----------------------------------------------------------------------------------------

\end{document}
